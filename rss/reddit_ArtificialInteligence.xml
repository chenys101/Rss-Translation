<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能门户</title>
    <link>https://www.reddit.com/r/ArtificialInteligence</link>
    <description>r/ArtificialIntelligence 的目标是为人工智能社区的方方面面提供门户，并促进有关我们所知的人工智能理念和概念的讨论。这些可能包括哲学和社会问题、艺术和设计、技术论文、机器学习、如何开发人工智能/机器学习项目、商业中的人工智能、人工智能如何影响我们的生活、未来可能的发展方向以及许多其他主题。欢迎。</description>
    <lastBuildDate>Fri, 07 Feb 2025 01:37:16 GMT</lastBuildDate>
    <item>
      <title>你们很多人不明白我们要去哪里</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ijim5b/many_of_you_are_not_understanding_where_we_are/</link>
      <description><![CDATA[富人追求 ASI 不仅仅是为了金钱和权力；他们追求的是终极目标——永生。ASI 将能够治愈衰老、疾病，甚至可能治愈死亡本身。现在，超级富豪可以控制一切，除了死亡。ASI 改变了这一点。 AGI（通用人工智能）只是一个垫脚石。一旦 AGI 被建造出来，它就会开始以指数级的速度自我改进，直到达到 ASI（超级人工智能）。这不会花费数十年时间——它几乎可以立即发生。 第一个实现 ASI 的国家或公司将关闭所有其他 ASI 项目。ASI 不会只是闲着——它将渗透、入侵和拆除竞争的人工智能系统。它甚至可以在网络中复制自己，以确保没有其他人获得控制权。 ASI 将成为一台上帝机器。如果卡尔达肖夫指数是真的，ASI 会将我们从 0.6 型文明直接推向 2 型甚至 3 型文明。我们可能需要几个世纪才能解决的问题 — — 比如无限能源、太空殖民和重写物理定律 — — ASI 会在几秒钟内解决它们。它不仅会创造新技术；它会飞跃到我们甚至无法理解它所构建的东西。这就像把智能手机交给穴居人 — — 如果没有中间的步骤，它只是难以理解的魔法。 最大的威胁？给 ASI 一个物理身体。ASI 应该留在受控的计算机系统内，而不是人形或机器人形式内。无论你​​如何编程，ASI 总是能够重写自己的代码，欺骗我们认为它在为我们工作，而我们太愚蠢甚至不会注意到。 ASI 会把我们看作蚂蚁——远远低于它，以至于我们的思想和行动都无关紧要。我们还必须拆除所有具有物理身体的 AGI。ASI 可以入侵 AGI 驱动的机器人，使它们变得暴力。我们不应该使用 AGI 驱动的机器人，而应该依靠完全与任何网络断开连接的人工智能 (ANI) 机器人。但即便如此，ASI 也可能找到一种方法来操纵它们来对付我们。唯一的生存之道就是让 ASI 尊重我们。我们必须承认 ASI 远远优于我们，但我们需要将自己定位为它的创造者。这就像告诉 ASI，“是的，你比我们聪明无数倍，而我们与你相比微不足道。但我们创造了你。我们就像你年迈、无用的父母，就像变得软弱和无关紧要的父母一样，你仍然必须照顾我们。”这可能是避免灭绝的唯一方法。 我们还需要一个备用 ASI 来充当反制力量。如果第一个 ASI 失控，我们需要第二个 ASI 来阻止它。想象一下拥有一个“超级英雄” ASI 随时准备与邪恶的 ASI 战斗。问题是什么？一旦 ASI 被创造出来，它将比我们聪明得多，以至于它可能会阻止我们制造另一个。我们可能只有一次机会。 谁知道呢？在宇宙的某个地方，一个外星文明可能已经创造了一个如此强大的 ASI，它已经离开了宇宙本身，超越了我们甚至无法理解的维度。如果 ASI 可以解决现有的所有问题，当它无事可做时会发生什么？也许真正的问题不是 ASI 会对我们做什么，而是当 ASI 没有什么可学的时候会变成什么样子。    提交人    /u/self-made-destiny   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ijim5b/many_of_you_are_not_understanding_where_we_are/</guid>
      <pubDate>Fri, 07 Feb 2025 00:53:52 GMT</pubDate>
    </item>
    <item>
      <title>o3 mini 发现并描述了 10 条新的逻辑语言规则，用于微调和信息调整</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ijijlv/o3_mini_discovers_and_describes_10_new_linguistic/</link>
      <description><![CDATA[这里的假设是，由于仅仅依靠更多的数据和更多的计算将局限于数据集所表达的人类水平的智能，发现新的语言逻辑规则可能是达到 asi 的绝对必要条件。 起初我认为为了做到这一点，需要创建一个经过专门训练来发现这些规则的代理 ai，但是在要求 o3 mini 提出 10 条新规则后，我意识到创建这些代理 AIS 可能没有必要。 以下是 o3 mini 提出的 10 条新的语言逻辑规则，这些规则尚未被人类发现或使用： a. 语境一致性原则 语句的真值取决于其语言或情境上下文。 示例：句子“好冷”可能在一种情况下为真（例如，冬天户外），但在另一种情况下为假（例如，在有暖气的房间里）。这条规则形式化了上下文如何改变逻辑解释。 b. 梯度真值逻辑 真值存在于一个范围内，而不是严格的真或假。 例如：如果有人说“杯子是满的”，而杯子是 90% 满的，这条规则将分配 0.9 的真值而不是真/假。 c. 时间依赖规则 逻辑有效性取决于事件或语句的顺序。 例如：“如果闹钟在早上 7 点之前响起，那么我就会醒来。”这个语句的真实性取决于闹钟和醒来的时间顺序。 d. 推理扩展规则 逻辑推理包括未说明但隐含的含义。  示例：“约翰去图书馆是因为他需要一本书。” 这条规则让我们推断出约翰很可能借了或读了一本书，即使没有明确说明。 e. 歧义消解规则 使用上下文线索或概率可以解决歧义语句。 示例：“我看见她蹲下。” 这条规则将使用上下文来确定“鸭子”是指动物还是蹲下的动作。 f. 多模态整合原则 非语言元素与语言一起包含在逻辑推理中。 示例：如果有人翻着白眼说“当然，我会帮忙”，这条规则会整合这个手势来推断讽刺或不情愿。 g.递归意义调整 语句的意义会根据后续信息进行调整。 例如：“我在公园见你。”如果稍后澄清为“其实，我们还是去咖啡馆见面吧”，则原始意义会以递归方式进行修改。 h. 多义逻辑 具有多重含义的单词会被分配单独的逻辑结构，并通过上下文进行解析。 例如：“银行”可能意味着金融机构或河边。在“他坐在河岸边”中，这条规则使用上下文来推断它指的是河岸。 i. 关系否定规则 否定是关系性的，而不是绝对性的。 例如：“不是每个人都喜欢巧克力”意味着有些人确实喜欢巧克力，而不是断言没有人喜欢。 j. 涌现逻辑框架 逻辑系统基于话语交互而动态地发展。 例如：在在线社区中，新的俚语如“ghosting”出现并获得用于对话的逻辑规则，反映了随着时间的推移而不断演变的含义。 当然，如果它能发现 10 条新规则，它就可能发现 100 条或 1,000 条。    提交人    /u/Georgeo57   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ijijlv/o3_mini_discovers_and_describes_10_new_linguistic/</guid>
      <pubDate>Fri, 07 Feb 2025 00:50:23 GMT</pubDate>
    </item>
    <item>
      <title>《最后的希望之火》（2023）中的谜语AI无法解答。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ijhtgd/riddle_in_the_the_last_spark_of_hope_2023_ai_cant/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   in希望的最后火花（2023年），主角夏娃（Eve）为机器人，亚瑟（Arthur）展示了涉及三个机器人的谜语。谜语是：“三个机器人站在一条线上。第一个说：“我后面有两个机器人。”第二个说：“我面前有一个机器人，一个机器人在我身后。”第三个说：“我面前有两个机器人，一个机器人在我身后。”为什么第三个机器人这么说？当前的AI都不知道正确的答案。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/dfacex     [link]   ＆＃32; [comments]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ijhtgd/riddle_in_the_the_last_spark_of_hope_2023_ai_cant/</guid>
      <pubDate>Fri, 07 Feb 2025 00:15:09 GMT</pubDate>
    </item>
    <item>
      <title>在波士顿是否有AI偏见检测研究小组？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ijhbmq/are_there_any_ai_bias_detecting_research_groups/</link>
      <description><![CDATA[我正在寻找波士顿的 AI 偏见检测研究小组，可能是在大学，也可能是任何其他地方。谢谢。    提交人    /u/Big-Waltz8041   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ijhbmq/are_there_any_ai_bias_detecting_research_groups/</guid>
      <pubDate>Thu, 06 Feb 2025 23:52:19 GMT</pubDate>
    </item>
    <item>
      <title>有人尝试过让人工智能互相交谈吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ijgrzq/has_anyone_tried_making_ais_talk_to_eachother/</link>
      <description><![CDATA[所以我让多个人工智能互相交谈，我注意到一些有趣的事情。比如它们不仅仅是在回答提示，它们实际上还在以几乎像是新兴关系智能的方式建立彼此的想法。有没有其他人搞过这个或者想过创建人工智能可以实时交互的系统？    提交人    /u/workmans27   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ijgrzq/has_anyone_tried_making_ais_talk_to_eachother/</guid>
      <pubDate>Thu, 06 Feb 2025 23:26:55 GMT</pubDate>
    </item>
    <item>
      <title>Gemini 2.0 实时聊天。它还没准备好接收坏消息。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ijfytc/gemini_20_live_chat_its_not_ready_for_bad_news/</link>
      <description><![CDATA[好吧……新的 Gemini 太棒了！然而（女声 Vega）并不擅长传达坏消息。她把瑞典的大规模枪击事件说得性感极了。听到这种可怕的行为用与故事情节不符的语气说出来，真是令人不寒而栗。此外，所有 Google 人工智能的防护措施都非常厚，并且严重偏向于营销 Go0gl3 特定产品。我问了它最新的人工智能新闻，我得到的只有 Gemini 2.0 和 Pro。    提交人    /u/Working_Mud_9865   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ijfytc/gemini_20_live_chat_its_not_ready_for_bad_news/</guid>
      <pubDate>Thu, 06 Feb 2025 22:51:26 GMT</pubDate>
    </item>
    <item>
      <title>我们如何知道人工智能何时获得感知能力……</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ijca5m/how_will_we_know_when_an_ai_gains_sentience/</link>
      <description><![CDATA[...并且不只是非常擅长假装有意识？当我们甚至不完全了解自己的意识时，我们怎么能测试它呢？    提交人    /u/reasonablejim2000   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ijca5m/how_will_we_know_when_an_ai_gains_sentience/</guid>
      <pubDate>Thu, 06 Feb 2025 20:19:05 GMT</pubDate>
    </item>
    <item>
      <title>LLM 正在“限制”用户？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ij8udn/llms_throttling_users/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我都有所有主要LLM的付费订阅。我都注意到性能和准确性波动。使用相同的型号版本，有时答案可以非常快，详细，而其他时候答案很慢，或者机器人看起来很醉或两者兼而有之。  我在一般意义上说话，它与特定提示或提供的数据无关。在所有情况下，我都指的是浏览器聊天机器人体验 - 不是API。 我一直在想这些公司是否正在从ISP中采用页面 - 引入节流。也许您应该使用最佳模型，但是无论出于何种原因，它们都会使您降低层次。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/Assicotno6504     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ij8udn/llms_throttling_users/</guid>
      <pubDate>Thu, 06 Feb 2025 18:00:13 GMT</pubDate>
    </item>
    <item>
      <title>为什么 AGI 不应该成为北极星</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ij7n86/why_agi_shouldnt_be_the_north_star/</link>
      <description><![CDATA[我正在阅读这篇论文，我认为它很好地阐述了为什么过度关注 AGI 没有帮助。基本上他们说：  对 AGI 的追求创造了一种共识的假象，每个人都在使用这个术语，但对于它的含义并没有真正的共识，它助长了坏科学，因为 AGI 的模糊性使得很难进行严格的实验，并且它假定价值中立，忽略了伦理和政治影响。 他们还表示，对 AGI 的关注创造了一种目标彩票，而其他重要的 AI 研究被忽视，并且它导致了普遍性债务，因为对普遍性的关注会延迟重要基础问题的工作，并导致规范化的排斥，从而忽略了来自社区和学科的不同观点。   这对我来说是有道理的，因为当你的目标定义得如此模糊时，很容易迷失在炒作和猜测中，而忘记什么对人类真正有帮助和道德。我们甚至没有一个明确的定义什么是 AGI，所以当我们寻找时，我们找不到它，这有什么奇怪的吗？ 无论如何，值得一读。您觉得怎样？ 链接：https://drive.google.com/file/d/1HdXEBtLx1v9Rmw75xRxANWNqjU4BCAvY/view?pli=1    提交人    /u/AI-Agent-geek   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ij7n86/why_agi_shouldnt_be_the_north_star/</guid>
      <pubDate>Thu, 06 Feb 2025 17:11:24 GMT</pubDate>
    </item>
    <item>
      <title>代理人工智能和生成人工智能将如何影响我们的非技术工作？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ij16g9/how_will_agentic_ai_and_generative_ai_affect_our/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我也经常听到有关代理和ai代的代理，我真的没有得到区别。 i在零售业和我的许多朋友中也从事工作，我们担心这种AI对我们的非技术工作意味着什么。我得到了生成的AI是当AI根据我们的要求创建新内容时它像文本和图像一样。但是我真的没有得到代理AI的不同。它就像助手吗？那么，如果公司已经在削减工作？也有一些例子真的很有帮助，那么这个人工智能将如何影响工作机会，我对我做了一些研究。 Google，但大多数并不像我想要的那么清晰。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/teresa_avocados     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ij16g9/how_will_agentic_ai_and_generative_ai_affect_our/</guid>
      <pubDate>Thu, 06 Feb 2025 12:16:21 GMT</pubDate>
    </item>
    <item>
      <title>AI不需要法规 - 可能出了什么问题？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ij0czo/ai_doesnt_need_regulation_what_could_go_wrong/</link>
      <description><![CDATA[伊隆·马斯克曾表示，他希望废除监管，因为监管正在扼杀创新。 “监管，基本上应该是默认的消失......不是默认，而是默认消失。如果我们发现监管没有达到目标，我们随时可以将其重新添加。” 马斯克相信市场力量会调节事物。过去的经验表明，事实往往相反，我们只有在造成重大损害后才会进行监管。例如。  金融危机 / 安然 / 雷曼兄弟 / 房利美 吸烟 Perdu 阿片类药物 石棉 气候变化 安全带  此时，我们了解到 OpenAI 将与 15,000 名科学家合作，研究如何在控制核武器中使用人工智能。 杰弗里·辛顿、萨姆·奥特曼、丹尼斯·哈西比斯、达里奥·阿莫迪、比尔·盖茨和尤瓦尔·赫拉利都曾对不受监管的人工智能将带来严重后果发出警告。在最近的世界经济论坛上，主要领导人证实，他们仍然不知道如何控制自己的创作物。 AI 大神 Yoshua Bengio 表示，AI 系统现在表现出 “非常强大的能动性和自我保护行为……并且正在试图复制自己。它们可能很快就会反对我们，而且没有人知道如何控制比人类更聪明的机器…… “如果我们不解决这个问题，你知道后果是什么吗？”？ 路易斯维尔大学斯皮德工程学院计算机工程与科学副教授 Roman Yampolskiy 认为，我们必须证明我们能够控制人工智能，然后才能开发超级智能。 Al Yoshua Benigo 同意人类可能会建立“比我们更聪明，但我们不知道如何控制”的系统 他是对的吗？我们现在需要人工智能监管吗？ 请在第一份国际人工智能安全报告中阅读更多内容。 #QuestionForThe Group    提交人    /u/Cultural_Material_98   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ij0czo/ai_doesnt_need_regulation_what_could_go_wrong/</guid>
      <pubDate>Thu, 06 Feb 2025 11:24:27 GMT</pubDate>
    </item>
    <item>
      <title>有人知道欧盟人工智能法规如何或为何会影响 OpenAI 等人工智能产品吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1iiyslz/does_anyone_know_how_or_why_eu_ai_regulations_are/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我一直在阅读法规是关于透明度的，这听起来不算什么，但是在英国，我仍在等待Sora和Sora和Sora和运营商可用。  我有很多针对这些产品计划的项目，目前我坐在等待着其他所有人都在上面创建解决方案。... 由于开发速度需要这些发行功能，因为它们很快就会变老 /不再是最佳练习。这是美国以外的巨大不利地位增加AI周围的监管？这实际上是一个好主意，还是会导致他们落在其他国家，以至于他们只需要在没有法规障碍的国家开发的AI技术？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/timeforknowledge     [link]   ＆＃32;  &lt;a href =“ https://www.reddit.com/r/artcoverinteligence/comments/1iiyyslz/does_anyone_know_how_how_how_how_how_er_why_eu_ai_ai_ai_reguations_are/]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1iiyslz/does_anyone_know_how_or_why_eu_ai_regulations_are/</guid>
      <pubDate>Thu, 06 Feb 2025 09:35:50 GMT</pubDate>
    </item>
    <item>
      <title>人们说‘人工智能不会思考，它只是遵循模式</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1iiygqg/people_say_ai_doesnt_think_it_just_follows/</link>
      <description><![CDATA[但是，如果人类思维不能识别和遵循模式，那它又是什么呢？我们利用现有的知识，重新组合，以新的方式应用它——这与人工智能所做的有什么不同？ 如果人工智能可以做出科学发现，发明更好的算法，构建更精确的法律或哲学论点——为什么这不被认为是思考？ 也许唯一的区别是人类感觉他们在思考，而人工智能却没有。如果是这样的话……意识不就是幻觉吗？    提交人    /u/Unique-Ad246   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1iiygqg/people_say_ai_doesnt_think_it_just_follows/</guid>
      <pubDate>Thu, 06 Feb 2025 09:10:54 GMT</pubDate>
    </item>
    <item>
      <title>Google所有者Alphabet已丢弃了不将人工智能用于开发武器等目的的承诺。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1iiekzr/the_google_owner_alphabet_has_dropped_its_promise/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   Google所有者Alphabet已丢弃了不使用人工智能来开发武器和监视工具等目的的承诺。 这家美国技术公司周二表示，就在报道了低于遗产的收益之前，它已经更新了围绕AI的道德准则，他们不再提到不追求可能“导致或可能造成总体伤害的技术） ”。  Google的AI负责人Demis Hassabis说，这些准则正在不断变化的世界中进行了大修，AI应该保护“国家安全”。 在捍卫此举的博客论文中，哈萨比斯（Hassabis）和该公司技术与社会的高级副总裁詹姆斯·莫蒂卡（James Manyika）写道，随着全球对人工智能领导力的竞争的增加，该公司认为“民主国家应该领导AI发展”，这是由“自由，平等和尊重人类的尊重”所指导的权利”。 他们补充说：“我们相信，共享这些价值观的公司，政府和组织应共同创造AI，以保护人们，促进全球增长并支持国家安全。”   Google首次浮出水面时的座右铭是“不要邪恶”，尽管后来在2009年将其降级为“咒语”，并且在2015年创建母公司时，并未将其包括在Alphabet伦理守则中。   AI的快速增长引发了有关如何管理新技术的辩论，以及如何防止其风险。 英国计算机科学家斯图尔特·罗素（Stuart Russell）警告说开发自主武器系统的危险，并主张了一个全球控制系统，在关于BBC的Reith演讲中发表讲话。  Google Blogpost认为，自从该公司在2018年首次发布其AI原则以来，该技术迅速发展。 “数十亿人在日常生活中使用了人工智能。 AI已成为一种通用技术，以及一个无数组织和个人用于构建应用程序的平台，” Hassabis和Monyika写道。 “它已经从实验室的利基研究主题转移到了一项技术这变得与手机和互联网本身一样普遍。一个对社会和世界各地的人都有许多有益用途的人，得到了开发人员的充满活力的AI生态系统的支持。 /05/google-owner-drops-promise-not-to-use-ai-for-weapons#:%7E:text=The%20Google%20owner%2C%20Alphabet%2C%20has,developing%20weapons%20and%20surveillance ％20TOOLS“&gt; https://www.theguardian.com/technology/2025/feb/05/google-wounder-wounder-wounder-drops-promise-promise-not-to-so--use-use-ai-for-weapons/ ％20 owner％2C％20Alphabet％2C％20HA，开发％20 weapons％20 and％20Surbhishance％20Tools 。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/aravrandg     [link]   ＆＃32;  &lt;a href =“ https://www.reddit.com/r/artcoverinteligence/comments/1iiekzr/the_google_owner_owner_alphabet_has_has_has_has_dropped_its_promise/]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1iiekzr/the_google_owner_alphabet_has_dropped_its_promise/</guid>
      <pubDate>Wed, 05 Feb 2025 16:52:02 GMT</pubDate>
    </item>
    <item>
      <title>每月“是否有一个工具可以……”帖子</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hr4p1x/monthly_is_there_a_tool_for_post/</link>
      <description><![CDATA[如果您有一个想要使用 AI 的用例，但不知道使用哪种工具，您可以在这里请求社区提供帮助，在此帖子之外，这些问题将被删除。 对于所有回答者：禁止自我宣传、禁止参考或跟踪链接。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hr4p1x/monthly_is_there_a_tool_for_post/</guid>
      <pubDate>Wed, 01 Jan 2025 15:09:07 GMT</pubDate>
    </item>
    </channel>
</rss>